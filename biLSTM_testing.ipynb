{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biLSTM_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHBHioL40_ju"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "from xgboost import XGBClassifier as XGB\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb"
      ],
      "metadata": {
        "id": "ug7AmKYn2pdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_unique_words = 10000\n",
        "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=n_unique_words)\n",
        "\n",
        "maxlen = 200\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test) \n",
        "\n",
        "# Defining a Bi-LSTM model.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model.summary()\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "\n",
        "# Defining the attention class.\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(attention,self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\n",
        "        super(attention,self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "# Here I have defined a class called attention in which I have defined two functions. Letâ€™s see what these functions will do for the mechanism.\n",
        "\n",
        "def build(self, input_shape):\n",
        "       self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
        "       self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\n",
        "    \n",
        "def call(self, x):\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = a\n",
        "        return output\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
        "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model2.add(attention(return_sequences=True)) # receive 3D and output 3D\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model2.summary()\n",
        "\n",
        "history3d=model2.fit(x_train, y_train,\n",
        "           batch_size=32,\n",
        "           epochs=12,\n",
        "           validation_data=[x_test, y_test])\n",
        "print(history3d.history['loss'])\n",
        "print(history3d.history['accuracy']) \n",
        "\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
        "model3.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model3.add(attention(return_sequences=False)) # receive 3D and output 3D\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model3.summary()\n",
        "\n",
        "history2d=model3.fit(x_train, y_train,\n",
        "           batch_size=32,\n",
        "           epochs=12,\n",
        "           validation_data=[x_test, y_test])\n",
        "print(history3d.history['loss'])\n",
        "print(history3d.history['accuracy'])"
      ],
      "metadata": {
        "id": "4PkqUTYi2PUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [0[1,2,3,...30],....862[]]\n",
        "# [                        ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=1, return_sequences=True, input_shape=(None,862,30))\n",
        "# 862\n"
      ],
      "metadata": {
        "id": "Op_bOHp8xksg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}